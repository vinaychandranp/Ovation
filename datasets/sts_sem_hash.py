import os
import pickle
import random
import datasets

import numpy as np

from random import randint
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

class STSSemHashDe(Dataset):
    """Face Landmarks dataset."""

    def __init__(self, subset='sts_small', mode='word', pad=0, raw=False,
                 tokenizer='nltk', lang='en', threshold=0.5,
                 split='train', rescale=(0.0, 1.0), max_instances=-1):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.dataset_name = 'Semantic Text Similarity - All'
        self.dataset_description = 'This dataset has been generated by ' \
               'merging MPD, SICK, Quora, StackExchange and and SemEval ' \
               'datasets. \n It has 258537 Training sentence pairs, 133102 ' \
               'Test sentence pairs and 59058 validation sentence pairs.'
        self.test_split = 'large'
        self.mode = '' if mode == 'word' else '_' + mode
        self.sample_type = mode
        self.tokenizer=tokenizer
        self.pad = pad
        self.raw = raw
        self.lang = lang
        self.rescale = rescale
        self.threshold = threshold
        self.dataset = subset
        self.max_instances = max_instances

        datasets.validate_rescale(self.rescale)
        self.dataset_path = os.path.join(datasets.data_root_directory,
                                         self.dataset)
        self.data_path = os.path.join(self.dataset_path, split,
                                      '{}.txt'.format(split))
        self.vocab_path = os.path.abspath(os.path.join(self.dataset_path,
                                                          'vocab{}.txt'.format(
                                                              self.mode)))
        self.metadata_path = os.path.abspath(os.path.join(self.dataset_path,
                                               'metadata{}.txt'.format(self.mode)))
        #self.w2v_path = os.path.join(self.dataset_path,
        #                             'w2v{}.npy'.format(self.mode))
        self.w2i, self.i2w = datasets.load_vocabulary(self.vocab_path)
        self.vocab_size = len(self.w2i)
        self.s1s, self.s2s, self.sims = self.load_data(self.data_path)
        self.data_len = len(self.s1s)

    def load_data(self, path):
        s1s, s2s, sims = [], [], []
        with open(path, 'r') as datafile:
            n_instance = 1
            for row in datafile:
                cols = row.strip().split('\t')
                try:
                    s1, s2, sim = cols[0], cols[1], float(cols[2])
                except:
                    continue
                s1 = datasets.split_sequence(s1, mode=self.sample_type,
                                             tokenizer=self.tokenizer,
                                             lang=self.lang)
                s2 = datasets.split_sequence(s2, mode=self.sample_type,
                                             tokenizer=self.tokenizer, lang=self.lang)
                if sim >= self.threshold:
                    s1s.append(s1)
                    s2s.append(s2)
                    sims.append(sim)
                if n_instance >= self.max_instances:
                    break
                n_instance += 1
        return s1s, s2s, sims


    def __len__(self):
        return len(self.s1s)

    def __getitem__(self, idx):
        neg_idx = randint(0, self.data_len)
        s1 = datasets.create_semantic_hashing(self.s1s[idx],
                                      self.w2i, self.raw, self.pad)
        s2 = datasets.create_semantic_hashing(self.s2s[idx],
                                              self.w2i, self.raw, self.pad)

        neg_sample = self.s1s[neg_idx] if random.random() >= 0.5 \
                                       else self.s1s[neg_idx]
        s3 = datasets.create_semantic_hashing(neg_sample,
                                              self.w2i, self.raw, self.pad)

        sample = {'s1': s1, 's2': s2, 's3': s3, 'sim': self.sims[idx]}
        return sample


if __name__ == '__main__':
    def batchify(batch):
        # batch will contain a list of {'src', 'target'}, or how you return it in load_func.

        # Implement method to batch the list above into Tensor here

        # assuming you already have two tensor containing batched Tensor for src and target
        return {'blah': batch}

    ds = STSSemHashDe(mode='semhash', lang='de', threshold=0.5, pad=20,
                      split='train', subset='GermanSTS', max_instances=100)

    dataloader = DataLoader(ds, batch_size=2,
                            shuffle=False, num_workers=4)

    for sample_batched in dataloader:
        print(sample_batched)